{
 "metadata": {
  "name": "",
  "signature": "sha256:7d60655fd40e070128a75b6211a4d2b811f213377ffedd460fc385c96bd08762"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab osx\n",
      "try:\n",
      "    reload(pvlparser)\n",
      "except:\n",
      "    \n",
      "    import pvlparser\n",
      "from ast import literal_eval\n",
      "from collections import OrderedDict, defaultdict\n",
      "from itertools import islice, combinations\n",
      "import json\n",
      "import re\n",
      "\n",
      "import numpy as np\n",
      "from scipy.sparse import csr_matrix, coo_matrix, dok_matrix\n",
      "import tables as tb\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "import ControlNetFileV0002_pb2 as spec\n",
      "\n",
      "import time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Mapping of the protobuf control network into an HDF5 table."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class ControlPoint(tb.IsDescription):\n",
      "    pointid = tb.StringCol(64)  #Why string?  #Int or float?  In the latter case, \n",
      "\n",
      "    pointtype = tb.StringCol(12)\n",
      "    choosername = tb.StringCol(36)\n",
      "    datetime = tb.StringCol(36)  #Should be datetime\n",
      "    referenceindex = tb.Int32Col()\n",
      "    apriorisurface_pointsource = tb.StringCol(32)\n",
      "    apriorisurface_pointfile = tb.StringCol(128)\n",
      "    aprioriradius_pointsource = tb.StringCol(32)\n",
      "    aprioriradius_pointfile = tb.StringCol(128)\n",
      "    \n",
      "    latitudeconstrained = tb.BoolCol()\n",
      "    longitudeconstrained = tb.BoolCol()\n",
      "    radiusconstrained = tb.BoolCol()\n",
      "    \n",
      "    #apriorilatitude = tb.FloatCol()\n",
      "    apriorix = tb.FloatCol()\n",
      "    #apriorilongitude = tb.FloatCol()\n",
      "    aprioriy = tb.FloatCol()\n",
      "    #aprioriradius = tb.FloatCol()\n",
      "    aprioriz = tb.FloatCol()\n",
      "    \n",
      "    #adjustedlatitude = tb.FloatCol()\n",
      "    adjustedx = tb.FloatCol()\n",
      "    #adjustedlongitude = tb.FloatCol()\n",
      "    adjustedy = tb.FloatCol()\n",
      "    #adjustedradius = tb.FloatCol()\n",
      "    adjustedz = tb.FloatCol()\n",
      "\n",
      "    #aprioricovar = tb.Float32Col(shape=6)\n",
      "    #adjustedcovar = tb.Float32Col(shape=6)\n",
      "\n",
      "    #Measure\n",
      "    serialnumber = tb.StringCol(256)\n",
      "    measuretype = tb.StringCol(32)\n",
      "    choosername = tb.StringCol(32)\n",
      "    datetime = tb.StringCol(64)  #should be datetime\n",
      "    editlock = tb.BoolCol()\n",
      "    ignore = tb.BoolCol()\n",
      "    jigsawrejected = tb.BoolCol()\n",
      "    diameter = tb.FloatCol()\n",
      "    sample = tb.FloatCol()\n",
      "    line = tb.FloatCol()\n",
      "    \n",
      "    samplesigma = tb.FloatCol()\n",
      "    linesigma = tb.FloatCol()\n",
      "    \n",
      "    apriorisample = tb.FloatCol()\n",
      "    aprioriline = tb.FloatCol()\n",
      "    sampleresidual = tb.FloatCol()\n",
      "    lineresidual = tb.FloatCol()\n",
      "    #goodnessoffit = tb.FloatCol()\n",
      "    #reference = tb.BoolCol()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Write the Protocol Buffer representation into the hdf5 container\n",
      "\n",
      "I think that this can be significantly faster using the Pandas storer class."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t1 = time.time()\n",
      "\n",
      "try:\n",
      "    h5file.close()\n",
      "except:\n",
      "    pass\n",
      "outfile = 'elysium.h5'\n",
      "h5file = tb.open_file(outfile, mode='w', title='cnet')\n",
      "image_group = h5file.create_group('/', 'Images', 'Lookup table of images to image serials')\n",
      "\n",
      "#Maybe kick down a level\n",
      "free_group = h5file.create_group('/', 'Free', 'Points of type free')\n",
      "#fixed_group = h5file.create_group('/', 'Fixed', 'Points of type fixed')\n",
      "#constrained_group = h5file.create_group('/', 'Constrained', 'Points of type constrained')\n",
      "\n",
      "#Table within groups\n",
      "freetable = h5file.create_table(free_group, 'controlmeasure', ControlPoint, 'Free Control Points')\n",
      "#constrainedtable = h5file.create_table(constrained_group, 'controlmeasure', ControlPoint, 'Constrainted Control Points')\n",
      "#fixedtable = h5file.create_table(fixed_group, 'controlmeasure', ControlPoint, 'Fixed Control Points')\n",
      "\n",
      "#For now - pack all points into a single free table.\n",
      "\n",
      "#Create a row object in the free table.\n",
      "freerow = freetable.row\n",
      "\n",
      "graph = defaultdict(int)\n",
      "images = set([])\n",
      "#Open the input dataset\n",
      "with open('Elysium_DayIR_Final.net', 'r') as stream:\n",
      "    header = spec.ControlNetFileHeaderV0002()\n",
      "    stream.seek(65536)  #Why are we doing more custom stuff?\n",
      "    headerbinary = stream.read(513513)\n",
      "    header.ParseFromString(headerbinary)\n",
      "    binarysizes = header.pointMessageSizes\n",
      "    for i, chunk in enumerate(binarysizes):\n",
      "        if i % 5000 == 0:\n",
      "            print i,\n",
      "        #Read the message\n",
      "        binary = stream.read(chunk)\n",
      "        cp = spec.ControlPointFileEntryV0002()\n",
      "        cp.ParseFromString(binary)\n",
      "        adj = []\n",
      "        #Loop over the points\n",
      "        for j, m in enumerate(cp.measures):\n",
      "            \n",
      "            freerow['pointid'] = cp.id\n",
      "            freerow['pointtype'] = cp.type\n",
      "            freerow['choosername'] = cp.chooserName\n",
      "            #datetime = tb.StringCol(36)  #Should be datetime\n",
      "            freerow['referenceindex'] = cp.referenceIndex\n",
      "            freerow['apriorisurface_pointsource'] = cp.aprioriSurfPointSource\n",
      "            freerow['apriorisurface_pointfile'] = cp.aprioriSurfPointSourceFile\n",
      "            freerow['aprioriradius_pointsource'] = cp.aprioriRadiusSource\n",
      "            freerow['aprioriradius_pointfile'] = cp.aprioriRadiusSourceFile\n",
      "\n",
      "            freerow['latitudeconstrained'] = cp.latitudeConstrained\n",
      "            freerow['longitudeconstrained'] = cp.longitudeConstrained\n",
      "            freerow['radiusconstrained'] = cp.radiusConstrained\n",
      "\n",
      "            #apriorilatitude = cp.\n",
      "            freerow['apriorix'] = cp.aprioriX\n",
      "            #apriorilongitude = tb.FloatCol()\n",
      "            freerow['aprioriy'] = cp.aprioriY\n",
      "            #aprioriradius = tb.FloatCol()\n",
      "            freerow['aprioriz'] = cp.aprioriZ\n",
      "\n",
      "            #adjustedlatitude = tb.FloatCol()\n",
      "            freerow['adjustedx'] = cp.adjustedX\n",
      "            #adjustedlongitude = tb.FloatCol()\n",
      "            freerow['adjustedy'] = cp.adjustedY\n",
      "            #adjustedradius = tb.FloatCol()\n",
      "            freerow['adjustedz'] = cp.adjustedZ\n",
      "            \n",
      "            '''\n",
      "            arr = cp.aprioriCovar\n",
      "            if len(arr) != 6:\n",
      "                freerow['aprioricovar'] = np.zeros(6)\n",
      "            else:\n",
      "                freerow['aprioricovar'] = arr\n",
      "            print freerow['aprioricovar']\n",
      "            \n",
      "            arr = cp.adjustedCovar\n",
      "            if len(arr) != 6:\n",
      "                freerow['adjustedcovar'] = np.zeros(6)\n",
      "            else:\n",
      "                freerow['adjustedcovar'] = arr\n",
      "\n",
      "            print freerow['adjustedcovar']\n",
      "            '''\n",
      "            \n",
      "            freerow['serialnumber'] = m.serialnumber\n",
      "            freerow['measuretype'] = m.type\n",
      "            freerow['choosername'] = m.choosername\n",
      "            #datetime = tb.StringCol(64)  #should be datetime\n",
      "            freerow['editlock'] = m.editLock\n",
      "            freerow['ignore'] = m.ignore\n",
      "            freerow['jigsawrejected'] = m.jigsawRejected\n",
      "            freerow['diameter'] = m.diameter\n",
      "            freerow['sample'] = m.sample\n",
      "            freerow['line'] = m.line\n",
      "\n",
      "            freerow['samplesigma'] = m.samplesigma\n",
      "            freerow['linesigma'] = m.linesigma\n",
      "            \n",
      "            freerow['apriorisample'] = m.apriorisample\n",
      "            freerow['aprioriline'] = m.aprioriline\n",
      "            freerow['sampleresidual'] = m.sampleResidual\n",
      "            freerow['lineresidual'] = m.lineResidual\n",
      "            \n",
      "            adj.append(m.serialnumber)\n",
      "            images.add(m.serialnumber)\n",
      "            \n",
      "            #Append the row to the table.  This is a bulk insert style\n",
      "            freerow.append()\n",
      "            \n",
      "            \n",
      "        for i,j in combinations(adj, 2):\n",
      "            graph[i,j] += 1\n",
      "            graph[j,i] += 1\n",
      "            \n",
      "        #Flush the table to explicitly write to disk\n",
      "        freetable.flush()\n",
      "images = list(images)\n",
      "t2 = time.time()\n",
      "print '\\nProtobuf to hdf5 took {}'.format(t2-t1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "15000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "20000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "25000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "30000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "35000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "40000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "45000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "50000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "55000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "60000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "65000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "70000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "75000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "80000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "85000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "90000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "95000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "100000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "105000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "110000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "115000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "120000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "125000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "135000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "140000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "145000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "150000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "155000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "160000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "165000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "170000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "175000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "180000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "185000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "190000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "195000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "200000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "205000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "210000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "215000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "220000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "225000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "230000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "235000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "240000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "245000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "250000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "255000 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Protobuf to hdf5 took 176.988647938\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def convert(term_dict):\n",
      "    ''' Convert a dictionary with elements of form ('d1', 't1'): 12 to a CSR type matrix.\n",
      "    The element ('d1', 't1'): 12 becomes entry (0, 0) = 12.\n",
      "    * Conversion from 1-indexed to 0-indexed.\n",
      "    * d is row\n",
      "    * t is column.\n",
      "    '''\n",
      "    # Create the appropriate format for the COO format.\n",
      "    data = []\n",
      "    row = []\n",
      "    col = []\n",
      "    for k, v in term_dict.items():\n",
      "        r = int(k[0])\n",
      "        c = int(k[1])\n",
      "        data.append(v)\n",
      "        row.append(r)\n",
      "        col.append(c)\n",
      "    # Create the COO-matrix\n",
      "    coo = coo_matrix((data,(row,col)))\n",
      "    # Let Scipy convert COO to CSR format and return\n",
      "    return csr_matrix(coo)\n",
      "\n",
      "def store_sparse_mat(m, name, store='store.h5'):\n",
      "    msg = \"This code only works for csr matrices\"\n",
      "    assert(m.__class__ == csr_matrix), msg\n",
      "    with tb.openFile(store,'a') as f:\n",
      "        for par in ('data', 'indices', 'indptr', 'shape'):\n",
      "            full_name = '%s_%s' % (name, par)\n",
      "            try:\n",
      "                n = getattr(f.root, full_name)\n",
      "                n._f_remove()\n",
      "            except AttributeError:\n",
      "                print \"ERROR\"\n",
      "\n",
      "            arr = np.array(getattr(m, par))\n",
      "            atom = tb.Atom.from_dtype(arr.dtype)\n",
      "            ds = f.createCArray(f.root, full_name, atom, arr.shape)\n",
      "            ds[:] = arr\n",
      "            ds.flush()\n",
      "        \n",
      "def load_sparse_mat(name, store='store.h5'):\n",
      "    with tb.openFile(store, 'a') as f:\n",
      "        pars = []\n",
      "        for par in ('data', 'indices', 'indptr', 'shape'):\n",
      "            pars.append(getattr(f.root, '%s_%s' % (name, par)).read())\n",
      "    m = csr_matrix(tuple(pars[:3]), shape=pars[3])\n",
      "    return m\n",
      "\n",
      "\n",
      "#Write the image lookup into the hdf5\n",
      "image_lookup = {}\n",
      "for i, img in enumerate(images):\n",
      "    image_lookup[img] = i\n",
      "    \n",
      "idxgraph = {}\n",
      "for k, v in graph.iteritems():\n",
      "    idxgraph[image_lookup[k[0]], image_lookup[k[1]]] = v\n",
      "\n",
      "csr_adjacencymatrix = convert(idxgraph)\n",
      "\n",
      "#Visualize the sparse matrix\n",
      "#spy(csr_adjacencymatrix)\n",
      "\n",
      "#Store the sparse adjacency matrix in hdf5 as CArrays\n",
      "store_sparse_mat(csr_adjacencymatrix, 'image_adj', store=outfile)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "elysium.h5\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Load h5 file into Pandas"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "outfile = 'elysium.h5'\n",
      "t1 = time.time()\n",
      "hdf = pd.read_hdf(outfile, '/Free/controlmeasure')\n",
      "t2 = time.time()\n",
      "print t2 - t1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "7.51281118393\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Measures sanity check\n",
      "assert(len(hdf == 841525)), 'Something went wrong'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 69
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Describe the control network\n",
      "\n",
      "What are the summary statistics for all numeric fields?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t1 = time.time()\n",
      "desc = hdf.describe()\n",
      "t2 = time.time()\n",
      "print t2 - t1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4.59186697006\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Get a subset of the table"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "desc.loc[:,['lineresidual', 'sampleresidual']]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>lineresidual</th>\n",
        "      <th>sampleresidual</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>count</th>\n",
        "      <td> 841542.000000</td>\n",
        "      <td> 8.415420e+05</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>mean</th>\n",
        "      <td>      0.000003</td>\n",
        "      <td> 3.362980e-09</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>std</th>\n",
        "      <td>      0.167996</td>\n",
        "      <td> 1.186296e-01</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>min</th>\n",
        "      <td>     -5.066055</td>\n",
        "      <td>-6.640318e+00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25%</th>\n",
        "      <td>     -0.092937</td>\n",
        "      <td>-6.806873e-02</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>50%</th>\n",
        "      <td>      0.000014</td>\n",
        "      <td>-1.661217e-04</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>75%</th>\n",
        "      <td>      0.092964</td>\n",
        "      <td> 6.753797e-02</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>max</th>\n",
        "      <td>      2.588894</td>\n",
        "      <td> 3.319423e+00</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "        lineresidual  sampleresidual\n",
        "count  841542.000000    8.415420e+05\n",
        "mean        0.000003    3.362980e-09\n",
        "std         0.167996    1.186296e-01\n",
        "min        -5.066055   -6.640318e+00\n",
        "25%        -0.092937   -6.806873e-02\n",
        "50%         0.000014   -1.661217e-04\n",
        "75%         0.092964    6.753797e-02\n",
        "max         2.588894    3.319423e+00"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Simple, Multi-part query (800,000+ points)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t1 = time.time()\n",
      "hdf.query('lineresidual < -0.09 | lineresidual > 0.09 | sampleresidual < -6.6 | sampleresidual > 0.067')\n",
      "t2 = time.time()\n",
      "print t2 - t1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.433874130249\n"
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Plot the scatter plot of the subset of the data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = hdf['lineresidual'].values\n",
      "y = hdf['sampleresidual'].values\n",
      "\n",
      "\n",
      "sigmas = 3\n",
      "stdx = np.std(x)\n",
      "stdy = np.std(y)\n",
      "\n",
      "meanx = np.mean(x)\n",
      "meany = np.mean(y)\n",
      "\n",
      "xlower = meanx - stdx * sigmas\n",
      "xupper = meanx + stdx * sigmas\n",
      "ylower = meany - stdy * sigmas\n",
      "yupper = meany + stdy * sigmas\n",
      "\n",
      "outlierx = np.where((x < xlower) | (x > xupper))[0]\n",
      "outliery = np.where((y < ylower) | (y > yupper))[0]\n",
      "print len(outlierx), len(outliery)\n",
      "mask = np.intersect1d(outlierx, outliery)\n",
      "        \n",
      "scatter(x[mask], y[mask], alpha=0.5)\n",
      "#hdf.plot(kind='scatter', x='lineresidual', y='sampleresidual')\n",
      "#show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "11500 9922\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 64,
       "text": [
        "<matplotlib.collections.PathCollection at 0x12ed813d0>"
       ]
      }
     ],
     "prompt_number": 64
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Aggregate by the pointid to count histograms"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Aggregate the data by pointid\n",
      "t1 = time.time()\n",
      "grouped = hdf.groupby('pointid')\n",
      "t2 = time.time()\n",
      "print \"Grouping required: {}\".format(t2 - t1)\n",
      "\n",
      "#Get a count of the number of control measures in each control point\n",
      "t1 = time.time()\n",
      "gcounts = grouped.size()\n",
      "t2 = time.time()\n",
      "print \"COUNT required : {}\".format(t2 - t1)\n",
      "\n",
      "hist(gcounts, np.arange(min(gcounts)-1, max(gcounts) + 1 ))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Grouping required: 0.000350952148438\n",
        "COUNT required : 0.923916101456"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 61,
       "text": [
        "(array([  0.00000000e+00,   8.12100000e+04,   8.01540000e+04,\n",
        "          5.49080000e+04,   2.72310000e+04,   9.95600000e+03,\n",
        "          2.78300000e+03,   3.66000000e+02,   8.00000000e+01]),\n",
        " array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),\n",
        " <a list of 9 Patch objects>)"
       ]
      }
     ],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print min(gcounts), max(gcounts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2 10\n"
       ]
      }
     ],
     "prompt_number": 161
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rawcounts = gcounts.values\n",
      "minc = np.min(rawcounts)\n",
      "maxc = np.max(rawcounts)\n",
      "median = np.median(rawcounts)\n",
      "std = np.std(rawcounts)\n",
      "hist(rawcounts, np.arange(minc, maxc+1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 166,
       "text": [
        "(array([  8.12100000e+04,   8.01540000e+04,   5.49080000e+04,\n",
        "          2.72310000e+04,   9.95600000e+03,   2.78300000e+03,\n",
        "          3.66000000e+02,   8.00000000e+01]),\n",
        " array([ 2,  3,  4,  5,  6,  7,  8,  9, 10]),\n",
        " <a list of 8 Patch objects>)"
       ]
      }
     ],
     "prompt_number": 166
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hist(rawcounts[rawcounts >= 5], np.arange(5, maxc+1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 168,
       "text": [
        "(array([ 27231.,   9956.,   2783.,    366.,     80.]),\n",
        " array([ 5,  6,  7,  8,  9, 10]),\n",
        " <a list of 5 Patch objects>)"
       ]
      }
     ],
     "prompt_number": 168
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Adjacency matrix and graph\n",
      "\n",
      "    1. Which images are connected?\n",
      "    2. By how many points?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Load the sparse adj matrix.\n",
      "#Unfortunately, if we want to append to this, we need to rewrite the whole thing to disk.  Maybe a better way?\n",
      "adjacency = load_sparse_mat('image_adj', store=outfile)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Visualize the sparse matrix\n",
      "spy(adjacency, markersize=2.5, precision=0.1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 48,
       "text": [
        "<matplotlib.lines.Line2D at 0x112a86390>"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('matrix.json', 'w') as jsonfile:\n",
      "    newspare = adjacency.copy()\n",
      "    adjacency[adjacency > 1] = 1\n",
      "    #newsparse[newsparse > 0] = 1\n",
      "    jsonfile.write(json.dumps(adjacency.todense().tolist()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 58
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## List intersection\n",
      "Given a list, elist, containing some number of pointids and an hdf5 table representing a control network, find the intersetion of the list with the control network.\n",
      "\n",
      "This is complete timing.  That is: (1) read the hdf5 file from disk, (2) read the text file from disk, (3) query the hdf5 table.  \n",
      "\n",
      "A similar operation using cnetextract required: 04:23:43.0"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t1 = time.time()\n",
      "outfile = 'elysium.h5'\n",
      "hdf = pd.read_hdf(outfile, '/Free/controlmeasure')\n",
      "elist = np.genfromtxt('Elysium_DayIR_Final_PtId_OddPlus.lis', dtype=str)\n",
      "res = hdf['pointid'].isin(elist)\n",
      "t2 = time.time()\n",
      "print t2 - t1\n",
      "\n",
      "print len(hdf[res].groupby('pointid'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "9.45865082741\n",
        "129694"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 62
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "cnetextract reports that 129694 points should be left after the set intersection.  \n",
      "\n",
      "Note: The hdf5 representation expands the nested representation, i.e. each row is a control measure and many control measures can have duplicate control point information.  This is not an issue because (1) disk is cheap, (2) hdf5 is chunkable and able to fit into RAM in sections (by extension, chunks can be defined at L1, L2, and (hardware dependent) L3 sizes), (3) compression should be used and control measure information is ripe for compression. \n",
      "\n",
      "Therefore, we must aggregrate the control measures into control points to get an accurate count."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t1 = time.time()\n",
      "len(qobj.groupby('pointid'))\n",
      "t2 = time.time()\n",
      "print t2 - t1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1.31991887093\n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "On my machine, the sum is < 10 seconds with no compression.  I do not image that, from an analytical standpoint, the aggregation is always required"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}